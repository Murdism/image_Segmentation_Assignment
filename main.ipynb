{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Import the necessary packages:**\n",
    "numpy, io, glob, tqdm_notebook, confusion_matrix, random, itertools, matplotlib.pyplot, torch, torch.nn,  torch.nn.functional, torch.utils.data, torch.optim, torch.optim.lr_scheduler, torch.nn.init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing \n",
    "from skimage import io\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler\n",
    "import torch.nn.init\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import tifffile as tiff  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **5. Initialization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IN_CHANNELS =  3                          # Number of input channels (e.g. RGB)\n",
    "MAIN_FOLDER  =    \"dataset/\"   # Replace with your \"/path/to/the/Images/folder/\"\n",
    "BATCH_SIZE =   10            # Number of samples in a mini-batch, example 10\n",
    "LABELS = [\"roads\", \"buildings\", \"low veg.\", \"trees\", \"cars\", \"clutter\"] # Label names\n",
    "N_CLASSES = len(LABELS)                   # Number of classes\n",
    "weights = torch.ones(N_CLASSES)           # Weights for class balancing\n",
    "DATA_FOLDER = MAIN_FOLDER + 'Images/Image_{}.tif'\n",
    "LABELS_FOLDER = MAIN_FOLDER + 'Labels/Label_{}.tif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **6. Functions you may need:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the standard ISPRS color palette\n",
    "palette = {0 : (255, 255, 255), # Impervious surfaces (white)\n",
    "           1 : (0, 0, 255),     # Buildings (blue)\n",
    "           2 : (0, 255, 255),   # Low vegetation (cyan)\n",
    "           3 : (0, 255, 0),     # Trees (green)\n",
    "           4 : (255, 255, 0),   # Cars (yellow)\n",
    "           5 : (255, 0, 0),     # Clutter (red)\n",
    "           6 : (0, 0, 0)}       # Undefined (black)\n",
    "invert_palette = {v: k for k, v in palette.items()}\n",
    "def convert_from_color(arr_3d, palette=invert_palette):\n",
    "    \"\"\" RGB-color encoding to grayscale labels \"\"\" '(From 0 to 6)'\n",
    "    arr_2d = np.zeros((arr_3d.shape[0], arr_3d.shape[1]), dtype=np.uint8)\n",
    "    for c, i in palette.items():\n",
    "        m = np.all(arr_3d == np.array(c).reshape(1, 1, 3), axis=2)\n",
    "        arr_2d[m] = i\n",
    "    return arr_2d\n",
    "class Load_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids):\n",
    "        super(Load_dataset, self).__init__()\n",
    "        # List of files\n",
    "        self.data_files = [DATA_FOLDER.format(id) for id in ids]\n",
    "        self.label_files = [LABELS_FOLDER.format(id) for id in ids]\n",
    "        # Sanity check : raise an error if some files do not exist\n",
    "        for f in self.data_files + self.label_files:\n",
    "            if not os.path.isfile(f):\n",
    "                raise KeyError('{} is not a file !'.format(f))\n",
    "    def __len__(self):\n",
    "        return len(self.data_files) # the length of the used data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         Pre-processing steps\n",
    "        #     # Data is normalized in [0, 1]\n",
    "        self.data = 1/255 * np.asarray(io.imread(self.data_files[idx]).transpose((2,0,1)), dtype='float32')\n",
    "        self.label = np.asarray(convert_from_color(io.imread(self.label_files[idx])), dtype='int64')\n",
    "        data_p, label_p = self.data,  self.label\n",
    "        # Return the torch.Tensor values\n",
    "        return (torch.from_numpy(data_p),\n",
    "                torch.from_numpy(label_p))\n",
    "def CrossEntropy2d(input, target, weight=None, size_average=True):\n",
    "    \"\"\" 2D version of the cross entropy loss \"\"\"\n",
    "    dim = input.dim()\n",
    "    if dim == 2:\n",
    "        return F.cross_entropy(input, target, weight, size_average)\n",
    "    elif dim == 4:\n",
    "        output = input.view(input.size(0), input.size(1), -1)\n",
    "        output = torch.transpose(output, 1, 2).contiguous()\n",
    "        output = output.view(-1, output.size(2))\n",
    "        target = target.view(-1)\n",
    "        return F.cross_entropy(output, target, weight, size_average)\n",
    "    else:\n",
    "        raise ValueError('Expected 2 or 4 dimensions (got {})'.format(dim))\n",
    "        \n",
    "def metrics(predictions, gts, label_values=LABELS):\n",
    "    cm = confusion_matrix(\n",
    "        gts,\n",
    "        predictions,\n",
    "        range(len(label_values)))\n",
    "    print(\"Confusion matrix :\")\n",
    "    print(cm)\n",
    "    print(\"---\")\n",
    "    # Compute global accuracy\n",
    "    total = sum(sum(cm))\n",
    "    accuracy = sum([cm[x][x] for x in range(len(cm))])\n",
    "    accuracy *= 100 / float(total)\n",
    "    print(\"{} pixels processed\".format(total))\n",
    "    print(\"Total accuracy : {}%\".format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # **7. Selecting training and testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids =list(range(0, 2000))\n",
    "test_ids =  list(range(2000,2400))\n",
    "train_data = Load_dataset(train_ids)\n",
    "test_data = Load_dataset(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Implement the Unet model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
