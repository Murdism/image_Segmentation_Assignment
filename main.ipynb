{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Import the necessary packages:**\n",
    "numpy, io, glob, tqdm_notebook, confusion_matrix, random, itertools, matplotlib.pyplot, torch, torch.nn,  torch.nn.functional, torch.utils.data, torch.optim, torch.optim.lr_scheduler, torch.nn.init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing \n",
    "from skimage import io\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import imagecodecs\n",
    "# %matplotlib inline\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler\n",
    "import torch.nn.init\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import tifffile as tiff  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **5. Initialization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IN_CHANNELS =  3                          # Number of input channels (e.g. RGB)\n",
    "MAIN_FOLDER  =    \"dataset/\"   # Replace with your \"/path/to/the/Images/folder/\"\n",
    "BATCH_SIZE =   10            # Number of samples in a mini-batch, example 10\n",
    "LABELS = [\"roads\", \"buildings\", \"low veg.\", \"trees\", \"cars\", \"clutter\"] # Label names\n",
    "N_CLASSES = len(LABELS)                   # Number of classes\n",
    "weights = torch.ones(N_CLASSES)           # Weights for class balancing\n",
    "DATA_FOLDER = MAIN_FOLDER + 'Images/Image_{}.tif'\n",
    "LABELS_FOLDER = MAIN_FOLDER + 'Labels/Label_{}.tif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **6. Functions you may need:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the standard ISPRS color palette\n",
    "palette = {0 : (255, 255, 255), # Impervious surfaces (white)\n",
    "           1 : (0, 0, 255),     # Buildings (blue)\n",
    "           2 : (0, 255, 255),   # Low vegetation (cyan)\n",
    "           3 : (0, 255, 0),     # Trees (green)\n",
    "           4 : (255, 255, 0),   # Cars (yellow)\n",
    "           5 : (255, 0, 0),     # Clutter (red)\n",
    "           6 : (0, 0, 0)}       # Undefined (black)\n",
    "invert_palette = {v: k for k, v in palette.items()}\n",
    "def convert_from_color(arr_3d, palette=invert_palette):\n",
    "    \"\"\" RGB-color encoding to grayscale labels \"\"\" '(From 0 to 6)'\n",
    "    arr_2d = np.zeros((arr_3d.shape[0], arr_3d.shape[1]), dtype=np.uint8)\n",
    "    for c, i in palette.items():\n",
    "        m = np.all(arr_3d == np.array(c).reshape(1, 1, 3), axis=2)\n",
    "        arr_2d[m] = i\n",
    "    return arr_2d\n",
    "class Load_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids):\n",
    "        super(Load_dataset, self).__init__()\n",
    "        # List of files\n",
    "        self.data_files = [DATA_FOLDER.format(id) for id in ids]\n",
    "        self.label_files = [LABELS_FOLDER.format(id) for id in ids]\n",
    "        # Sanity check : raise an error if some files do not exist\n",
    "        for f in self.data_files + self.label_files:\n",
    "            if not os.path.isfile(f):\n",
    "                raise KeyError('{} is not a file !'.format(f))\n",
    "    def __len__(self):\n",
    "        return len(self.data_files) # the length of the used data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         Pre-processing steps\n",
    "        #     # Data is normalized in [0, 1]\n",
    "        self.data = 1/255 * np.asarray(io.imread(self.data_files[idx]).transpose((2,0,1)), dtype='float32')\n",
    "        self.label = np.asarray(convert_from_color(io.imread(self.label_files[idx])), dtype='int64')\n",
    "        data_p, label_p = self.data,  self.label\n",
    "        # Return the torch.Tensor values\n",
    "        return (torch.from_numpy(data_p),\n",
    "                torch.from_numpy(label_p))\n",
    "def CrossEntropy2d(input, target, weight=None, size_average=True):\n",
    "    \"\"\" 2D version of the cross entropy loss \"\"\"\n",
    "    dim = input.dim()\n",
    "    if dim == 2:\n",
    "        return F.cross_entropy(input, target, weight, size_average)\n",
    "    elif dim == 4:\n",
    "        output = input.view(input.size(0), input.size(1), -1)\n",
    "        output = torch.transpose(output, 1, 2).contiguous()\n",
    "        output = output.view(-1, output.size(2))\n",
    "        target = target.view(-1)\n",
    "        return F.cross_entropy(output, target, weight, size_average)\n",
    "    else:\n",
    "        raise ValueError('Expected 2 or 4 dimensions (got {})'.format(dim))\n",
    "        \n",
    "def metrics(predictions, gts, label_values=LABELS):\n",
    "    cm = confusion_matrix(\n",
    "        gts,\n",
    "        predictions,\n",
    "        range(len(label_values)))\n",
    "    print(\"Confusion matrix :\")\n",
    "    print(cm)\n",
    "    print(\"---\")\n",
    "    # Compute global accuracy\n",
    "    total = sum(sum(cm))\n",
    "    accuracy = sum([cm[x][x] for x in range(len(cm))])\n",
    "    accuracy *= 100 / float(total)\n",
    "    print(\"{} pixels processed\".format(total))\n",
    "    print(\"Total accuracy : {}%\".format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # **7. Selecting training and testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids =list(range(0, 2000))\n",
    "test_ids =  list(range(2000,2400))\n",
    "train_data = Load_dataset(train_ids)\n",
    "test_data = Load_dataset(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Implement the Unet model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3,padding='same'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3,padding='same'),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "def crop_tensor(input,target):\n",
    "    diff =input.size()[2] - target.size()[2] # difference in width\n",
    "    diff = diff // 2\n",
    "    return(input[:,:,diff:input.size()[2]-diff,diff:input.size()[2]-diff])\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,IN_CHANNELS=3,OUT_CHANNELS=1):\n",
    "        super().__init__() \n",
    "        self.encode_conv1 = conv_block(IN_CHANNELS, 64)\n",
    "        self.encode_conv2 = conv_block(64, 128)\n",
    "        self.encode_conv3 = conv_block(128, 256)\n",
    "       # self.encode_conv4 = conv_block(256, 512)\n",
    "       # self.encode_conv5 = conv_block(512, 1024)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "       # self.conv_transpose1 = nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=2,stride=2)\n",
    "       # self.decode_conv1 = conv_block(1024, 512)\n",
    "       # self.conv_transpose2 = nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=2,stride=2)\n",
    "       # self.decode_conv2 = conv_block(512, 256)\n",
    "        self.conv_transpose3 = nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=2,stride=2)\n",
    "        self.decode_conv3 = conv_block(256, 128)\n",
    "        self.conv_transpose4 = nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=2,stride=2)\n",
    "        self.decode_conv4 = conv_block(128, 64)\n",
    "        self.out = nn.Conv2d(in_channels=64,out_channels=OUT_CHANNELS,kernel_size=1)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.encode_conv1(x)\n",
    "        x2 = self.maxpool(x1)\n",
    "        x3 = self.encode_conv2(x2)\n",
    "        x4 = self.maxpool(x3)\n",
    "        x5 = self.encode_conv3(x4)\n",
    "        # x6 = self.maxpool(x5)\n",
    "        # x7 = self.encode_conv4(x6)\n",
    "       # x8 = self.maxpool(x7)\n",
    "       # x9 = self.encode_conv5(x8)\n",
    "       # print(\" out : \",out.size())\n",
    "        print(\" x1 : \",x1.size())\n",
    "        print(\" x2 : \",x2.size())\n",
    "        print(\" x3 : \",x3.size())\n",
    "        print(\" x4 : \",x4.size())\n",
    "        print(\" x5 : \",x5.size())\n",
    "        # print(\" x6 : \",x6.size())\n",
    "        # print(\" x7 : \",x7.size())\n",
    "\n",
    "        \n",
    "        # Decoder x1,x3,x5,x7 will be used as input \n",
    "       # x10 = self.conv_transpose1(x9)\n",
    "       # x7_cropped = crop_tensor(x7,x10)\n",
    "       # x7_10 = torch.cat([x10,x7_cropped],1)\n",
    "       # x11 = self.decode_conv1(x7_10)\n",
    "       # x12 = self.conv_transpose2(x11) \n",
    "        # print(\" x12 : \",x12.size())\n",
    "        # x5_cropped = crop_tensor(x5,x12)\n",
    "        # print(\" x5 : \",x5.size())\n",
    "        # print(\" x5_cropped : \",x5_cropped.size())\n",
    "        # x5_12 = torch.cat([x12,x5_cropped],dim=1)\n",
    "        # x13 = self.decode_conv2(x5_12)\n",
    "        x14 = self.conv_transpose3(x5) #x13\n",
    "        x3_cropped = crop_tensor(x3,x14)\n",
    "        x3_14 = torch.cat([x14,x3_cropped],dim=1)\n",
    "        x15 = self.decode_conv3(x3_14)\n",
    "        x16 = self.conv_transpose4(x15)\n",
    "        x1_cropped = crop_tensor(x1,x16)\n",
    "        x1_16 = torch.cat([x16,x1_cropped],dim=1)\n",
    "        x17 = self.decode_conv4(x1_16)\n",
    "        out = self.out(x17)\n",
    "\n",
    "        # print(\" x8 : \",x8.size())\n",
    "        # print(\" x9 : \",x9.size())\n",
    "        # print(\" x10 : \",x10.size())\n",
    "        # print(\" x11 : \",x11.size())\n",
    "        # print(\" x12 : \",x12.size())\n",
    "        # print(\" x13 : \",x13.size())\n",
    "        print(\" x14 : \",x14.size())\n",
    "        print(\" x15 : \",x15.size())\n",
    "        print(\" x16 : \",x16.size())\n",
    "        print(\" x17 : \",x17.size())\n",
    "        \n",
    "        return out.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Load_dataset"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_data)\n",
    "images, labels = dataiter.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_data):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        print(\"inputs : \",inputs.size())\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        print(\"inputs unsqueezed : \",inputs.size())\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs).squeeze()\n",
    "        print(\"outputs : \",outputs.size())\n",
    "        print(\"labels : \",labels.size())\n",
    "        # Compute the loss and its gradients\n",
    "        loss = CrossEntropy2d(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_data) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs :  torch.Size([3, 300, 300])\n",
      "inputs unsqueezed :  torch.Size([1, 3, 300, 300])\n",
      " x1 :  torch.Size([1, 64, 300, 300])\n",
      " x2 :  torch.Size([1, 64, 150, 150])\n",
      " x3 :  torch.Size([1, 128, 150, 150])\n",
      " x4 :  torch.Size([1, 128, 75, 75])\n",
      " x5 :  torch.Size([1, 256, 75, 75])\n",
      " x14 :  torch.Size([1, 128, 150, 150])\n",
      " x15 :  torch.Size([1, 128, 150, 150])\n",
      " x16 :  torch.Size([1, 64, 300, 300])\n",
      " x17 :  torch.Size([1, 64, 300, 300])\n",
      "outputs :  torch.Size([300, 300])\n",
      "labels :  torch.Size([300, 300])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/segmentation/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_one_epoch(\u001b[39m0\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn [40], line 22\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabels : \u001b[39m\u001b[39m\"\u001b[39m,labels\u001b[39m.\u001b[39msize())\n\u001b[1;32m     21\u001b[0m \u001b[39m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m loss \u001b[39m=\u001b[39m CrossEntropy2d(outputs\u001b[39m.\u001b[39;49msqueeze(), labels)\n\u001b[1;32m     23\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     25\u001b[0m \u001b[39m# Adjust learning weights\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [3], line 43\u001b[0m, in \u001b[0;36mCrossEntropy2d\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m     41\u001b[0m dim \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m dim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight, size_average)\n\u001b[1;32m     44\u001b[0m \u001b[39melif\u001b[39;00m dim \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m     45\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mview(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/segmentation/lib/python3.10/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "train_one_epoch(0, None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('segmentation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c6cf681ed52ccdf5461de7f00a5396296db5e0f2a58b4d6a57334ba109c144f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
