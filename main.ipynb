{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Import the necessary packages:**\n",
    "numpy, io, glob, tqdm_notebook, confusion_matrix, random, itertools, matplotlib.pyplot, torch, torch.nn,  torch.nn.functional, torch.utils.data, torch.optim, torch.optim.lr_scheduler, torch.nn.init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avl1/anaconda3/envs/segmentation/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing \n",
    "from skimage import io\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import imagecodecs\n",
    "# %matplotlib inline\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler\n",
    "import torch.nn.init\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import tifffile as tiff  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **5. Initialization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IN_CHANNELS =  3                          # Number of input channels (e.g. RGB)\n",
    "MAIN_FOLDER  =    \"dataset/\"   # Replace with your \"/path/to/the/Images/folder/\"\n",
    "BATCH_SIZE =   10            # Number of samples in a mini-batch, example 10\n",
    "LABELS = [\"roads\", \"buildings\", \"low veg.\", \"trees\", \"cars\", \"clutter\"] # Label names\n",
    "N_CLASSES = len(LABELS)                   # Number of classes\n",
    "weights = torch.ones(N_CLASSES)           # Weights for class balancing\n",
    "DATA_FOLDER = MAIN_FOLDER + 'Images/Image_{}.tif'\n",
    "LABELS_FOLDER = MAIN_FOLDER + 'Labels/Label_{}.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **6. Functions you may need:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the standard ISPRS color palette\n",
    "palette = {0 : (255, 255, 255), # Impervious surfaces (white)\n",
    "           1 : (0, 0, 255),     # Buildings (blue)\n",
    "           2 : (0, 255, 255),   # Low vegetation (cyan)\n",
    "           3 : (0, 255, 0),     # Trees (green)\n",
    "           4 : (255, 255, 0),   # Cars (yellow)\n",
    "           5 : (255, 0, 0),     # Clutter (red)\n",
    "           6 : (0, 0, 0)}       # Undefined (black)\n",
    "invert_palette = {v: k for k, v in palette.items()}\n",
    "def convert_from_color(arr_3d, palette=invert_palette):\n",
    "    \"\"\" RGB-color encoding to grayscale labels \"\"\" '(From 0 to 6)'\n",
    "    arr_2d = np.zeros((arr_3d.shape[0], arr_3d.shape[1]), dtype=np.uint8)\n",
    "    for c, i in palette.items():\n",
    "        m = np.all(arr_3d == np.array(c).reshape(1, 1, 3), axis=2)\n",
    "        arr_2d[m] = i\n",
    "    return arr_2d\n",
    "class Load_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids):\n",
    "        super(Load_dataset, self).__init__()\n",
    "        # List of files\n",
    "        self.data_files = [DATA_FOLDER.format(id) for id in ids]\n",
    "        self.label_files = [LABELS_FOLDER.format(id) for id in ids]\n",
    "        # Sanity check : raise an error if some files do not exist\n",
    "        for f in self.data_files + self.label_files:\n",
    "            if not os.path.isfile(f):\n",
    "                raise KeyError('{} is not a file !'.format(f))\n",
    "    def __len__(self):\n",
    "        return len(self.data_files) # the length of the used data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         Pre-processing steps\n",
    "        #     # Data is normalized in [0, 1]\n",
    "        self.data = 1/255 * np.asarray(io.imread(self.data_files[idx]).transpose((2,0,1)), dtype='float32')\n",
    "        self.label = np.asarray(convert_from_color(io.imread(self.label_files[idx])), dtype='int64')\n",
    "        data_p, label_p = self.data,  self.label\n",
    "        # Return the torch.Tensor values\n",
    "        return (torch.from_numpy(data_p),\n",
    "                torch.from_numpy(label_p))\n",
    "def CrossEntropy2d(input, target, weight=weights.to(device), size_average=True):\n",
    "    \"\"\" 2D version of the cross entropy loss \"\"\"\n",
    "    dim = input.dim()\n",
    "    if dim == 2:\n",
    "        return F.cross_entropy(input, target, weight, size_average)\n",
    "    elif dim == 4:\n",
    "        #print(f\"input 1: {input.size()}\")\n",
    "        output = input.view(input.size(0), input.size(1), -1)\n",
    "        #print(f\"input 2: {output.size()}\")\n",
    "        output = torch.transpose(output, 1, 2).contiguous()\n",
    "        #print(f\"input 3 Transposed: {output.size()}\")\n",
    "        output = output.view(-1, output.size(2))\n",
    "        #print(f\"input 4: {output.size()}\")\n",
    "        #print(f\"target: {target.size()}\")\n",
    "        target = target.view(-1)\n",
    "        #print(f\"Target last: {target.size()}\")\n",
    "        return F.cross_entropy(output, target, weight, size_average)\n",
    "    else:\n",
    "        raise ValueError('Expected 2 or 4 dimensions (got {})'.format(dim))\n",
    "        \n",
    "def metrics(predictions, gts, label_values=LABELS):\n",
    "    predictions = torch.argmax(predictions,dim =1 )\n",
    "    cm = confusion_matrix(\n",
    "        gts,\n",
    "        predictions,\n",
    "        labels = 6) #range(len(label_values))\n",
    "    print(\"Confusion matrix :\")\n",
    "    print(cm)\n",
    "    print(\"---\")\n",
    "    # Compute global accuracy\n",
    "    total = sum(sum(cm))\n",
    "    accuracy = sum([cm[x][x] for x in range(len(cm))])\n",
    "    accuracy *= 100 / float(total)\n",
    "    print(\"{} pixels processed\".format(total))\n",
    "    print(\"Total accuracy : {}%\".format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # **7. Selecting training and testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids =list(range(0, 2000))\n",
    "test_ids =  list(range(2000,2400))\n",
    "train_data = Load_dataset(train_ids)\n",
    "test_data = Load_dataset(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torch.utils.data.DataLoader(train_data,\n",
    "                                             batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                             num_workers=8)\n",
    "testing_data = torch.utils.data.DataLoader(test_data,\n",
    "                                             batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                             num_workers=8)\n",
    "                                            \n",
    "                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Implement the Unet model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3,padding='same'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3,padding='same'),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "def crop_tensor(input,target):\n",
    "    diff =input.size()[2] - target.size()[2] # difference in width\n",
    "    diff = diff // 2\n",
    "    return(input[:,:,diff:input.size()[2]-diff,diff:input.size()[2]-diff])\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,IN_CHANNELS=3,OUT_CHANNELS=6):\n",
    "        super().__init__() \n",
    "        self.encode_conv1 = conv_block(IN_CHANNELS, 64)\n",
    "        self.encode_conv2 = conv_block(64, 128)\n",
    "        self.encode_conv3 = conv_block(128, 256)\n",
    "        self.encode_conv4 = conv_block(256, 512)\n",
    "        self.encode_conv5 = conv_block(512, 1024)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv_transpose1 = nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=2,stride=2)\n",
    "        self.decode_conv1 = conv_block(1024, 512)\n",
    "        self.conv_transpose2 = nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=2,stride=2)\n",
    "        self.decode_conv2 = conv_block(512, 256)\n",
    "        self.conv_transpose3 = nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=2,stride=2)\n",
    "        self.decode_conv3 = conv_block(256, 128)\n",
    "        self.conv_transpose4 = nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=2,stride=2)\n",
    "        self.decode_conv4 = conv_block(128, 64)\n",
    "        self.out = nn.Conv2d(in_channels=64,out_channels=OUT_CHANNELS,kernel_size=1)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = x.to(device)\n",
    "        x1 = self.encode_conv1(x).to(device)\n",
    "        x2 = self.maxpool(x1).to(device)\n",
    "        x3 = self.encode_conv2(x2).to(device)\n",
    "        x4 = self.maxpool(x3).to(device)\n",
    "        x5 = self.encode_conv3(x4).to(device)\n",
    "        x6 = self.maxpool(x5)\n",
    "        x7 = self.encode_conv4(x6)\n",
    "        x8 = self.maxpool(x7)\n",
    "        x9 = self.encode_conv5(x8)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Decoder x1,x3,x5,x7 will be used as input \n",
    "        p1d = (0, 1,0,1) # pad last dim by 1 on rigt and bottom side\n",
    "        x10 = self.conv_transpose1(x9).to(device)\n",
    "        x10 = F.pad(x10, p1d, \"constant\", 1).to(device)  # effectively zero padding\n",
    "        x7_10 = torch.cat([x10,x7],dim=1).to(device)\n",
    "        x11 = self.decode_conv1(x7_10).to(device)\n",
    "        x12 = self.conv_transpose2(x11) .to(device) # x11\n",
    "        x12 = F.pad(x12, p1d, \"constant\", 1).to(device)  # effectively zero padding\n",
    "        x5_12 = torch.cat([x12,x5],dim=1).to(device)\n",
    "        x13 = self.decode_conv2(x5_12).to(device)\n",
    "        x14 = self.conv_transpose3(x13).to(device) \n",
    "        #x14 = F.pad(x14, p1d, \"constant\", 1)  # effectively zero padding\n",
    "        x3_14 = torch.cat([x14,x3],dim=1).to(device)\n",
    "        x15 = self.decode_conv3(x3_14).to(device)\n",
    "        x16 = self.conv_transpose4(x15).to(device)\n",
    "        #x16 = F.pad(x10, p1d, \"constant\", 1)  # effectively zero padding\n",
    "        x1_16 = torch.cat([x16,x1],dim=1).to(device)\n",
    "        x17 = self.decode_conv4(x1_16).to(device)\n",
    "        out = self.out(x17).to(device)\n",
    "        return out.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs = 5):\n",
    "    running_loss = 0.\n",
    "    epoch_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for epoch in range(epochs): \n",
    "        Num_batches = 0\n",
    "        for i, data in enumerate(training_data):\n",
    "            # Every data instance is an input + label pair\n",
    "            inputs, labels = data\n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs).to(device)\n",
    "            \n",
    "            # print(\"labels : \",type(labels))\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            labels = labels.unsqueeze(0).to(device)\n",
    "            # print(\"Outputs : \",outputs.size())\n",
    "            # print(\"Labels : \",labels.size())\n",
    "            # Compute the loss and its gradients\n",
    "            loss = CrossEntropy2d(outputs,labels).to(device)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "            # if i%10 ==0:\n",
    "            #     print(f\"Loss {i} : {loss}\")\n",
    "\n",
    "            # Gather data and report\n",
    "            # running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            Num_batches+=1\n",
    "            \n",
    "            # if i % 10 == 9:\n",
    "            #     last_loss = running_loss / 10 # loss per batch\n",
    "            #     print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            #     # tb_x = epoch_index * len(train_data) + i + 1\n",
    "            #     # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            #     running_loss = 0.\n",
    "        epoch_loss = epoch_loss/Num_batches\n",
    "        last_loss += epoch_loss\n",
    "        print('  Epoch {} loss: {}'.format(epoch + 1, epoch_loss))\n",
    "    return last_loss/epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 loss: 0.8976705557107926\n",
      "  Epoch 2 loss: 0.8649155038028955\n",
      "  Epoch 3 loss: 0.8418127617922573\n",
      "  Epoch 4 loss: 0.8238594995546774\n",
      "  Epoch 5 loss: 0.8000463318162205\n",
      "  Epoch 6 loss: 0.7852357566710974\n",
      "  Epoch 7 loss: 0.7588630473317488\n",
      "  Epoch 8 loss: 0.7405515373526522\n",
      "  Epoch 9 loss: 0.7150818236474089\n",
      "  Epoch 10 loss: 0.6967560731509347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7924792890830685"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Num_epochs = 10\n",
    "train(Num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(output,gt):\n",
    "    with torch.no_grad():\n",
    "        correct = torch.eq(output,gt).int()\n",
    "        accuracy = float(correct.sum())/float(correct.numel())\n",
    "        return accuracy,correct,correct.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy in batch: 73.90305277777777\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for i, data in enumerate(testing_data):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.detach().cpu()\n",
    "        \n",
    "        #metrics(outputs,labels)\n",
    "        outputs = torch.argmax(outputs,dim=1)\n",
    "        accuracy,tempcorrect,tempincorrect = pixel_accuracy(outputs,labels)\n",
    "        correct = correct + tempcorrect\n",
    "        incorrect = incorrect + tempincorrect\n",
    "    acc_total = float(correct.sum())/float(incorrect)\n",
    "    print(f\" Accuracy in batch: {acc_total*100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('segmentation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c6cf681ed52ccdf5461de7f00a5396296db5e0f2a58b4d6a57334ba109c144f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
